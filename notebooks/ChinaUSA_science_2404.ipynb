{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pyalex\n",
    "from pyalex import Works, Institutions, Topics, Funders\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import scipy\n",
    "pyalex.config.email = \"aymann.mhammedi@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a list of topics\n",
    "topics = Topics().get()\n",
    "\n",
    "# Print the list of topics with their IDs and display names\n",
    "#for topic in topics:\n",
    "#   print(f\"ID: {topic['id']}, Display Name: {topic['display_name']}, Description: {topic['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'display_name', 'alternate_titles', 'country_code', 'description', 'homepage_url', 'image_url', 'image_thumbnail_url', 'grants_count', 'works_count', 'cited_by_count', 'summary_stats', 'ids', 'counts_by_year', 'roles', 'updated_date', 'created_date'])\n"
     ]
    }
   ],
   "source": [
    "funders = Funders().get()\n",
    "# List fields of funders object\n",
    "print(funders[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filter criteria for works published in the USA in 2015\n",
    "filter_criteria = {\n",
    "    \"from_publication_date\": \"2016-01-01\",  # Start date\n",
    "    \"to_publication_date\": \"2020-12-31\",    # End date\n",
    "    \"institutions\": {\"country_code\": \"us\"},  # Filter for U.S.-based institutions\n",
    "    \"primary_location\": {\"source\": {\"type\": \"journal\"}},  # Filter for journal articles\n",
    "    \"primary_topic\": {\"subfield\": {\"id\": 2740}},  # Filter for Pulmonary and Respiratory Medicine\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch the works with pagination\n",
    "query = Works().filter(**filter_criteria)\n",
    "all_works_16_20 = list(chain.from_iterable(query.paginate(per_page=200, n_max=None)))  # Adjust per_page as needed, n_max=None for all papers (heavy)\n",
    "\n",
    "# Count the number of works\n",
    "num_works_16_20 = len(all_works_16_20)\n",
    "\n",
    "# Print the number of works\n",
    "print(f\"Number of works published in the USA in between 2016-2020: {num_works_16_20}\")\n",
    "\n",
    "# Optionally, print the first few works to verify\n",
    "for work in all_works_16_20[:5]:  # Print the first 5 works as an example\n",
    "    print(f\"ID: {work['id']}, Title: {work['title']}, Publication Year: {work['publication_year']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all_works_2018 with only works from 2018\n",
    "all_works_2018 = [work for work in all_works_16_20 if work['publication_year'] == 2018]\n",
    "num_works = len(all_works_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique work IDs\n",
    "unique_work_ids = len(set(work['id'] for work in all_works_2018))\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of unique work IDs: {unique_work_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration coauthorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a collaboration network with country information\n",
    "G_collaboration = nx.Graph()\n",
    "\n",
    "# Add nodes and edges for authors\n",
    "for work in all_works_2018:\n",
    "    authors = [\n",
    "        (authorship['author']['id'], authorship['countries'])  # Assuming the first country is the primary one\n",
    "        for authorship in work['authorships']\n",
    "    ]\n",
    "    for i, (author1, country1) in enumerate(authors):\n",
    "        # Add author1 node with country attribute\n",
    "        G_collaboration.add_node(author1, country=country1)\n",
    "        for author2, country2 in authors[i + 1:]:\n",
    "            # Add author2 node with country attribute\n",
    "            G_collaboration.add_node(author2, country=country2)\n",
    "            # Add edge between author1 and author2\n",
    "            if G_collaboration.has_edge(author1, author2):\n",
    "                G_collaboration[author1][author2]['weight'] += 1\n",
    "            else:\n",
    "                G_collaboration.add_edge(author1, author2, weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COAUTHORSHIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes and edges in the collaboration network\n",
    "num_nodes = G_collaboration.number_of_nodes()\n",
    "num_edges = G_collaboration.number_of_edges()\n",
    "print(f\"Number of authors in the collaboration network: {num_nodes}\")\n",
    "print(f\"Number of edges in the collaboration network: {num_edges}\")\n",
    "# Mean and median edges per node\n",
    "mean_edges_per_node = np.mean([G_collaboration.degree(n) for n in G_collaboration.nodes()])\n",
    "median_edges_per_node = np.median([G_collaboration.degree(n) for n in G_collaboration.nodes()])\n",
    "print(f\"Mean number of coauthors per author: {mean_edges_per_node}\")\n",
    "print(f\"Median number of coauthors per author: {median_edges_per_node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centrality\n",
    "#degree_centrality = nx.degree_centrality(G_collaboration)\n",
    "#top_collaborators = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "#print(\"Top collaborative authors by degree centrality:\", top_collaborators)\n",
    "\n",
    "# Betweenness centrality\n",
    "#betweenness_centrality = nx.betweenness_centrality(G_collaboration)\n",
    "#top_intermediaries = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "#print(\"Top intermediaries in the network by betweenness centrality:\", top_intermediaries)\n",
    "\n",
    "# Connected components\n",
    "#connected_components = list(nx.connected_components(G_collaboration))\n",
    "#print(f\"Number of connected components: {len(connected_components)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of coauthors for each work\n",
    "coauthor_counts = Counter(len(work['authorships']) for work in all_works_2018)\n",
    "\n",
    "# Prepare data for plotting\n",
    "coauthor_numbers = list(coauthor_counts.keys())\n",
    "paper_counts = list(coauthor_counts.values())\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(coauthor_numbers, paper_counts, color='skyblue')\n",
    "plt.xlabel('Number of Coauthors')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.title('Number of Papers for Each Number of Coauthors')\n",
    "plt.xticks(coauthor_numbers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean and median number of coauthors per paper\n",
    "mean_coauthors = np.mean([len(work['authorships']) for work in all_works_2018])\n",
    "median_coauthors = np.median([len(work['authorships']) for work in all_works_2018])\n",
    "print(f\"Mean number of coauthors per paper: {mean_coauthors:.2f}\")\n",
    "print(f\"Median number of coauthors per paper: {median_coauthors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure to China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'arrive pas à ajouter le pays dans le graph des auteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct a graph of funders and papers (nodes are funders and papers, edges are funding)\n",
    "G_works = nx.Graph()\n",
    "# Add nodes for each work and its funders\n",
    "for work in all_works_2018:\n",
    "    work_id = work['id']\n",
    "    G_works.add_node(work_id, type='work')\n",
    "    for grant in work['grants']:\n",
    "        funder_name = grant['funder_display_name']\n",
    "        G_works.add_node(funder_name, type='funder')\n",
    "        G_works.add_edge(work_id, funder_name)\n",
    "        # Add country to the funder type nodes\n",
    "        if 'country_code' in grant:\n",
    "            G_works.nodes[funder_name]['country'] = grant['country_code']\n",
    "        else:\n",
    "            # If country_code is not available, set a default value or skip\n",
    "            G_works.nodes[funder_name]['country'] = 'Unknown'\n",
    "# Add attributes to work nodes (citation count, has_chinese_coauthor, has_grant, etc.)\n",
    "for work in all_works_2018:\n",
    "    work_id = work['id']\n",
    "    G_works.nodes[work_id]['cited_by_count'] = work['cited_by_count']\n",
    "    G_works.nodes[work_id]['has_grant'] = len(work['grants']) > 0\n",
    "# Add nodes for authors and edges between authors and papers\n",
    "    for authorship in work['authorships']:\n",
    "        author_id = authorship['author']['id']\n",
    "        G_works.add_node(author_id, type='author')\n",
    "        G_works.add_edge(work_id, author_id)\n",
    "# Add attribute to work nodes (has_chinese_coauthor, has_not_american_coauthor, etc.)\n",
    "    G_works.nodes[work_id]['has_not_american_coauthor'] = any('US' not in authorship['countries'] for authorship in work['authorships'])\n",
    "    G_works.nodes[work_id]['has_chinese_coauthor'] = any('CN' in authorship['countries'] for authorship in work['authorships'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### International collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of work nodes with only american coauthors, at least 1 foreign coauthor, at least 1 chinese coauthor\n",
    "share_american_coauthors = sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor']) / len(all_works_2018)\n",
    "share_foreign_coauthors = sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 0 ) / len(all_works_2018)\n",
    "share_chinese_coauthors = sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor']) / len(all_works_2018)\n",
    "print(f\"Share of works with only American coauthors: {share_american_coauthors:.2%}\")\n",
    "print(f\"Share of works with at least one foreign coauthor: {share_foreign_coauthors:.2%}\")\n",
    "print(f\"Share of works with at least one Chinese coauthor: {share_chinese_coauthors:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and median citation counts for works with or without international coauthors\n",
    "mean_citations_with_international = np.mean([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 0])\n",
    "mean_citations_without_international = np.mean([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor']])\n",
    "median_citations_with_international = np.median([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 0])\n",
    "median_citations_without_international = np.median([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor']])\n",
    "print(f\"Mean citation count for works with international coauthors: {mean_citations_with_international:.2f}\")\n",
    "print(f\"Mean citation count for works without international coauthors: {mean_citations_without_international:.2f}\")\n",
    "print(f\"Median citation count for works with international coauthors: {median_citations_with_international:.2f}\")\n",
    "print(f\"Median citation count for works without international coauthors: {median_citations_without_international:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and median citation counts for works with and without chinese coauthors\n",
    "mean_citations_with_chinese = np.mean([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor']])\n",
    "mean_citations_without_chinese = np.mean([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor'] == 0])\n",
    "median_citations_with_chinese = np.median([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor']])\n",
    "median_citations_without_chinese = np.median([G_works.nodes[work['id']]['cited_by_count'] for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor'] == 0])\n",
    "print(f\"Mean citation count for works with Chinese coauthors: {mean_citations_with_chinese:.2f}\")\n",
    "print(f\"Mean citation count for works without Chinese coauthors: {mean_citations_without_chinese:.2f}\")\n",
    "print(f\"Median citation count for works with Chinese coauthors: {median_citations_with_chinese:.2f}\")\n",
    "print(f\"Median citation count for works without Chinese coauthors: {median_citations_without_chinese:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among US-only papers, at least 1 foreign coauthor, at least 1 Chinese coauthor: what share has a grant?\n",
    "share_grant_with_international = sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 0 and G_works.nodes[work['id']]['has_grant']) / sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 0)\n",
    "share_grant_without_international = sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 1 and G_works.nodes[work['id']]['has_grant']) / sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_not_american_coauthor'] == 1)\n",
    "share_grant_with_chinese = sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor'] and G_works.nodes[work['id']]['has_grant']) / sum(1 for work in all_works_2018 if G_works.nodes[work['id']]['has_chinese_coauthor'])\n",
    "\n",
    "print(f\"Share of works with international coauthors that have a grant: {share_grant_with_international:.2%}\")\n",
    "print(f\"Share of works without international coauthors that have a grant: {share_grant_without_international:.2%}\")\n",
    "print(f\"Share of works with Chinese coauthors that have a grant: {share_grant_with_chinese:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For works with at least 1 chinese coauthor, who are the funders?\n",
    "USCN_funder_counts = Counter()\n",
    "for work in all_works_2018:\n",
    "    if G_works.nodes[work['id']]['has_chinese_coauthor']:\n",
    "        for grant in work['grants']:\n",
    "            funder_name = grant['funder_display_name']\n",
    "            USCN_funder_counts[funder_name] += 1\n",
    "# Print the top 10 funders for works with at least one Chinese coauthor\n",
    "print(\"Funders for works with at least one Chinese coauthor:\")\n",
    "for funder, count in USCN_funder_counts.most_common(10):  # Print top 10 funders\n",
    "    print(f\"{funder}: {count} works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For works with at least 1 chinese coauthor, from which countries are the funders?\n",
    "USCN_funder_country_counts = Counter()\n",
    "for work in all_works_2018:\n",
    "    if G_works.nodes[work['id']]['has_chinese_coauthor']:\n",
    "        for grant in work['grants']:\n",
    "            funder_country = G_works.nodes[grant['funder_display_name']]['country']\n",
    "            USCN_funder_country_counts[funder_country] += 1\n",
    "# Print the top 10 funder countries for works with at least one Chinese coauthor\n",
    "print(\"Funder countries for works with at least one Chinese coauthor:\")\n",
    "for country, count in USCN_funder_country_counts.most_common(10):  # Print top 10 funder countries\n",
    "    print(f\"{country}: {count} works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density of the subgraph\n",
    "density = nx.density(us_china_collaboration)\n",
    "print(f\"Density of U.S.-China collaboration subgraph: {density}\")\n",
    "\n",
    "# Bridges in the subgraph\n",
    "bridges = list(nx.bridges(us_china_collaboration))\n",
    "print(f\"Number of bridges in U.S.-China collaboration subgraph: {len(bridges)}\")\n",
    "# edges whose removal disconnects the graph to find critical collaborations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of unique institutions\n",
    "unique_institutions = set()\n",
    "for work in all_works_2018:\n",
    "    for author in work['authorships']:\n",
    "        for institution in author['institutions']:\n",
    "            unique_institutions.add(institution['id'])\n",
    "print(f\"Total number of unique institutions: {len(unique_institutions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of unique institutions per paper\n",
    "institution_counts = Counter(len(work['authorships'][0]['institutions']) for work in all_works_2018)\n",
    "institution_numbers = list(institution_counts.keys())\n",
    "institution_paper_counts = list(institution_counts.values())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(institution_numbers, institution_paper_counts, color='salmon')\n",
    "plt.xlabel('Number of Institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each work generate dummy variable if at least one coauthor is from china, add it to all_works_2018\n",
    "for work in all_works_2018:\n",
    "    work['has_chinese_coauthor'] = any('CN' in author['countries'] for author in work['authorships'])\n",
    "# Count the number of works with at least one Chinese coauthor\n",
    "num_works_with_chinese_coauthor = sum(work['has_chinese_coauthor'] for work in all_works_2018)\n",
    "print(f\"Number of works with at least one Chinese coauthor: {num_works_with_chinese_coauthor}\")\n",
    "print(f\"Percentage of works with at least one Chinese coauthor: {num_works_with_chinese_coauthor / num_works * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of Chinese coauthors\n",
    "chinese_coauthors = sum(\n",
    "    1\n",
    "    for work in all_works_2018\n",
    "    for authorship in work['authorships']\n",
    "    for inst in authorship.get('institutions', [])\n",
    "    if inst.get('country_code') == 'CN'\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of Chinese coauthors: {chinese_coauthors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each work generate dummy variable if all authors are from USA, add it to all_works_2018\n",
    "for work in all_works_2018:\n",
    "    work['all_authors_from_usa'] = all('US' in author['countries'] for author in work['authorships'])\n",
    "# Count the number of works with all authors from the USA\n",
    "num_works_all_authors_from_usa = sum(work['all_authors_from_usa'] for work in all_works_2018)\n",
    "print(f\"Number of works with all authors from the USA: {num_works_all_authors_from_usa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUALITY OF RESEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min max mean and deciles of cited_by_count\n",
    "cited_by_counts = [work['cited_by_count'] for work in all_works_2018]\n",
    "min_cited_by_count = min(cited_by_counts)\n",
    "max_cited_by_count = max(cited_by_counts)\n",
    "mean_cited_by_count = np.mean(cited_by_counts)\n",
    "deciles = np.percentile(cited_by_counts, np.arange(0, 101, 10))\n",
    "print(f\"Min cited_by_count: {min_cited_by_count}\")\n",
    "print(f\"Max cited_by_count: {max_cited_by_count}\")\n",
    "print(f\"Mean cited_by_count: {mean_cited_by_count:.2f}\")\n",
    "print(f\"Deciles of cited_by_count: {deciles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of citations per paper\n",
    "citations_counts = Counter(work['cited_by_count'] for work in all_works_2018)\n",
    "# Prepare data for plotting\n",
    "citation_numbers = list(citations_counts.keys())\n",
    "citation_paper_counts = list(citations_counts.values())\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 2000)\n",
    "plt.bar(citation_numbers, citation_paper_counts, color='lightgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean citations for paperss with and without Chinese coauthors\n",
    "mean_citations_with_chinese = np.mean([work['cited_by_count'] for work in all_works_2018 if work['has_chinese_coauthor']])\n",
    "mean_citations_without_chinese = np.mean([work['cited_by_count'] for work in all_works_2018 if not work['has_chinese_coauthor']])\n",
    "print(f\"Mean citations for papers with Chinese coauthors: {mean_citations_with_chinese:.2f}\")\n",
    "print(f\"Mean citations for papers without Chinese coauthors: {mean_citations_without_chinese:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPOSURE TO POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of papers per author\n",
    "author_paper_counts = Counter(\n",
    "    authorship['author']['id']\n",
    "    for work in all_works_2018\n",
    "    for authorship in work['authorships']\n",
    ")\n",
    "\n",
    "# Prepare data for plotting\n",
    "authors = list(author_paper_counts.keys())\n",
    "paper_counts = list(author_paper_counts.values())\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(paper_counts, bins=range(1, max(paper_counts) + 2), color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Number of Papers')\n",
    "plt.ylabel('Number of Authors')\n",
    "plt.title('Distribution of Number of Papers per Author')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each US author compute share of papers with Chinese coauthors\n",
    "for work in all_works_2018:\n",
    "    for authorship in work['authorships']:\n",
    "        if 'US' in authorship['countries']:\n",
    "            authorship['has_chinese_coauthor'] = any('CN' in author['countries'] for author in work['authorships'])\n",
    "# Count the number of papers for each US author\n",
    "us_author_paper_counts = defaultdict(int)\n",
    "for work in all_works_2018:\n",
    "    for authorship in work['authorships']:\n",
    "        if 'US' in authorship['countries']:\n",
    "            us_author_paper_counts[authorship['author']['id']] += 1\n",
    "# Compute the share of papers with Chinese coauthors for each US author\n",
    "us_author_chinese_coauthor_counts = defaultdict(int)\n",
    "for work in all_works_2018:\n",
    "    for authorship in work['authorships']:\n",
    "        if 'US' in authorship['countries'] and authorship.get('has_chinese_coauthor'):\n",
    "            us_author_chinese_coauthor_counts[authorship['author']['id']] += 1\n",
    "# Compute the share of papers with Chinese coauthors for each US author\n",
    "us_author_shares = {\n",
    "    author_id: us_author_chinese_coauthor_counts[author_id] / count\n",
    "    for author_id, count in us_author_paper_counts.items()\n",
    "}\n",
    "# Prepare data for plotting\n",
    "us_author_ids = list(us_author_shares.keys())\n",
    "us_author_share_values = list(us_author_shares.values())\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(us_author_share_values, bins=20, color='salmon', edgecolor='black')\n",
    "plt.xlabel('Share of Papers with Chinese Coauthors')\n",
    "plt.ylabel('Number of US Authors')\n",
    "plt.title('Distribution of Share of Papers with Chinese Coauthors for US Authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List US authors with share of papers with Chinese coauthors > 0.5\n",
    "high_share_authors = [author_id for author_id, share in us_author_shares.items() if share > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHINA INITIATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filter criteria for works published in the USA in 2020\n",
    "filter_criteria = {\n",
    "    \"publication_year\": 2019,  # Filter for works published in 2020\n",
    "    \"institutions\": {\"country_code\": \"us\"},\n",
    "    \"primary_location\": {\"source\": {\"type\": \"journal\"}},  # Filter for journal articles\n",
    "    #\"primary_topic\": {\"domain\": {\"id\": 4}},  # Filter for Health science\n",
    "    \"primary_topic\": {\"subfield\": {\"id\": 2740}},  # Filter for Pulmonary and Respiratory Medicine\n",
    "}\n",
    "\n",
    "# Fetch the works with pagination\n",
    "query = Works().filter(**filter_criteria)\n",
    "all_works_2019 = list(chain.from_iterable(query.paginate(per_page=200, n_max=None)))  # Adjust per_page as needed, n_max=None for all papers (heavy)\n",
    "\n",
    "# Count the number of works\n",
    "num_works = len(all_works_2019)\n",
    "\n",
    "# Print the number of works\n",
    "print(f\"Number of works published in the USA in 2020: {num_works}\")\n",
    "\n",
    "# Optionally, print the first few works to verify\n",
    "for work in all_works_2019[:5]:  # Print the first 5 works as an example\n",
    "    print(f\"ID: {work['id']}, Title: {work['title']}, Publication Year: {work['publication_year']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each work generate dummy variable if at least one coauthor is from china, add it to all_works_2020\n",
    "for work in all_works_2019:\n",
    "    work['has_chinese_coauthor'] = any('CN' in author['countries'] for author in work['authorships'])\n",
    "# Count the number of works with at least one Chinese coauthor\n",
    "num_works_with_chinese_coauthor = sum(work['has_chinese_coauthor'] for work in all_works_2019)\n",
    "print(f\"Number of works with at least one Chinese coauthor: {num_works_with_chinese_coauthor}\")\n",
    "print(f\"Percentage of works with at least one Chinese coauthor: {num_works_with_chinese_coauthor / num_works * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_works_2019[0]['grants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_works_2019[0]['grants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variable for each work if it has a grant\n",
    "for work in all_works_2019:\n",
    "    work['has_grant'] = len(work['grants']) > 0\n",
    "# Count the number of works with at least one grant\n",
    "num_works_with_grant = sum(work['has_grant'] for work in all_works_2019)\n",
    "print(f\"Number of works with at least one grant: {num_works_with_grant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size of grants per paper\n",
    "grant_counts = Counter(len(work['grants']) for work in all_works_2019)\n",
    "# Prepare data for plotting\n",
    "grant_numbers = list(grant_counts.keys())\n",
    "grant_paper_counts = list(grant_counts.values())\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(grant_numbers, grant_paper_counts, color='lightgreen')\n",
    "plt.xlabel('Number of Grants')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.title('Number of Papers for Each Number of Grants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprend pas pourquoi ca marche pas (voir cellule suivante -> il y a grants)\n",
    "all_works_2019['id' == 'https://openalex.org/W2990041408']['has_grant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print works with grants\n",
    "#for work in all_works_2019:\n",
    "#   if len(work['grants']) > 0:\n",
    "#        print(f\"ID: {work['id']}, Has Grant: {work['has_grant']}, Grant: {work['grants']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print funder names and their counts\n",
    "funder_counts = Counter()\n",
    "for work in all_works_2019:\n",
    "    for grant in work['grants']:\n",
    "        funder_counts[grant['funder_display_name']] += 1\n",
    "# Print the funder names and their counts\n",
    "for funder, count in funder_counts.most_common(10):  # Print top 10 funders\n",
    "    print(f\"Funder: {funder}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of funders in funder_counts\n",
    "num_funders = len(funder_counts)\n",
    "print(f\"Number of funders: {num_funders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top funders of works with Chinese coauthors vs without Chinese coauthors\n",
    "funder_counts_with_chinese = Counter()\n",
    "funder_counts_without_chinese = Counter()\n",
    "for work in all_works_2019:\n",
    "    for grant in work['grants']:\n",
    "        if work['has_chinese_coauthor']:\n",
    "            funder_counts_with_chinese[grant['funder_display_name']] += 1\n",
    "        else:\n",
    "            funder_counts_without_chinese[grant['funder_display_name']] += 1\n",
    "# Print the top funders for works with Chinese coauthors\n",
    "print(\"Top funders for works with Chinese coauthors:\")\n",
    "for funder, count in funder_counts_with_chinese.most_common(10):  # Print top 10 funders\n",
    "    print(f\"Funder: {funder}, Count: {count}\")\n",
    "# Print the top funders for works without Chinese coauthors\n",
    "print(\"Top funders for works without Chinese coauthors:\")\n",
    "for funder, count in funder_counts_without_chinese.most_common(10):  # Print top 10 funders\n",
    "    print(f\"Funder: {funder}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top funders of works with Chinese coauthors vs without Chinese coauthors\n",
    "funder_counts_with_chinese = Counter()\n",
    "funder_counts_without_chinese = Counter()\n",
    "for work in all_works_2018:\n",
    "\n",
    "    for grant in work['grants']:\n",
    "        if work['has_chinese_coauthor']:\n",
    "            funder_counts_with_chinese[grant['funder_display_name']] += 1\n",
    "        else:\n",
    "            funder_counts_without_chinese[grant['funder_display_name']] += 1\n",
    "# Print the top funders for works with Chinese coauthors\n",
    "print(\"Top funders for works with Chinese coauthors:\")\n",
    "for funder, count in funder_counts_with_chinese.most_common(10):  # Print top 10 funders\n",
    "    print(f\"Funder: {funder}, Count: {count}\")\n",
    "# Print the top funders for works without Chinese coauthors\n",
    "print(\"Top funders for works without Chinese coauthors:\")\n",
    "for funder, count in funder_counts_without_chinese.most_common(10):  # Print top 10 funders\n",
    "    print(f\"Funder: {funder}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing more info on fundings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of funder nodes in the graph\n",
    "num_funder_nodes = sum(1 for node, data in G_fundings.nodes(data=True) if data['type'] == 'funder')\n",
    "print(f\"Number of funder nodes in the graph: {num_funder_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of author nodes in the graph\n",
    "num_author_nodes = sum(1 for node, data in G_fundings.nodes(data=True) if data['type'] == 'author')\n",
    "print(f\"Number of author nodes in the graph: {num_author_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of authors for each work and store in all_works_2019\n",
    "for work in all_works_2019:\n",
    "    work['num_authors'] = len(work['authorships'])\n",
    "#get total number of authors\n",
    "total_coauthors = sum(work['num_authors'] for work in all_works_2019)\n",
    "#get total of unique authors\n",
    "unique_authors = set()\n",
    "for work in all_works_2019:\n",
    "    for author in work['authorships']:\n",
    "        unique_authors.add(author['author']['id'])\n",
    "\n",
    "print(f\"Total number of authors: {total_coauthors}\")\n",
    "print(f\"Total number of unique authors: {len(unique_authors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SURPLUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique country codes of co-authors\n",
    "country_codes = set(\n",
    "    inst.get('country_code') \n",
    "    for work in all_works_2018 \n",
    "    for authorship in work['authorships'] \n",
    "    for inst in authorship.get('institutions', []) \n",
    "    if 'country_code' in inst\n",
    ")\n",
    "\n",
    "# Print the unique country codes\n",
    "print(f\"Different country codes of co-authors: {country_codes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count institutions in the US and outside the US\n",
    "us_institutions = set(\n",
    "    inst['id']\n",
    "    for work in all_works_2018\n",
    "    for authorship in work['authorships']\n",
    "    for inst in authorship.get('institutions', [])\n",
    "    if inst.get('country_code') == 'US'\n",
    ")\n",
    "\n",
    "non_us_institutions = set(\n",
    "    inst['id']\n",
    "    for work in all_works_2018\n",
    "    for authorship in work['authorships']\n",
    "    for inst in authorship.get('institutions', [])\n",
    "    if inst.get('country_code') != 'US'\n",
    ")\n",
    "\n",
    "cn_institutions = set(\n",
    "    inst['id']\n",
    "    for work in all_works_2018\n",
    "    for authorship in work['authorships']\n",
    "    for inst in authorship.get('institutions', [])\n",
    "    if inst.get('country_code') == 'CN'\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of institutions in the US: {len(us_institutions)}\")\n",
    "print(f\"Number of institutions outside the US: {len(non_us_institutions)}\")\n",
    "print(f\"Number of institutions in China: {len(cn_institutions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing first order effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ========================\n",
    "# Step 1: Select Top 10% of China-collaborating US researchers (before Nov 2018)\n",
    "# ========================\n",
    "\n",
    "# Build US authors + share of papers with Chinese coauthors\n",
    "us_author_total_papers_pre = defaultdict(int)\n",
    "us_author_chinese_papers_pre = defaultdict(int)\n",
    "\n",
    "for work in all_works_16_20:  # assuming you already have all works 2016-2020\n",
    "    if work['publication_year'] < 2019:  # Before Nov 2018 => anything up to 2018\n",
    "        for authorship in work['authorships']:\n",
    "            if 'US' in authorship['countries']:\n",
    "                us_author_total_papers_pre[authorship['author']['id']] += 1\n",
    "                if any('CN' in coauth['countries'] for coauth in work['authorships']):\n",
    "                    us_author_chinese_papers_pre[authorship['author']['id']] += 1\n",
    "\n",
    "# Compute share of China collaboration\n",
    "us_author_china_share = {\n",
    "    author_id: us_author_chinese_papers_pre[author_id] / us_author_total_papers_pre[author_id]\n",
    "    for author_id in us_author_total_papers_pre\n",
    "    if us_author_total_papers_pre[author_id] >= 2  # filter for active researchers\n",
    "}\n",
    "\n",
    "# Find top 10% most China-collaborative authors\n",
    "threshold = np.percentile(list(us_author_china_share.values()), 90)\n",
    "top_china_authors = {author_id for author_id, share in us_author_china_share.items() if share >= threshold}\n",
    "\n",
    "print(f\"Selected {len(top_china_authors)} top China-collaborating US authors.\")\n",
    "\n",
    "# ========================\n",
    "# Step 2: Track outcomes before vs after Nov 2018\n",
    "# ========================\n",
    "\n",
    "# Initialize metrics\n",
    "author_metrics = defaultdict(lambda: {\n",
    "    'pre_pubs': 0, 'post_pubs': 0,\n",
    "    'pre_china_pubs': 0, 'post_china_pubs': 0,\n",
    "    'pre_other_foreign_pubs': 0, 'post_other_foreign_pubs': 0,\n",
    "    'pre_citations': [], 'post_citations': []\n",
    "})\n",
    "\n",
    "for work in all_works_16_20:\n",
    "    year = work['publication_year']\n",
    "    if year < 2016 or year > 2020:\n",
    "        continue  # skip\n",
    "\n",
    "    is_pre = (year <= 2018)  # Pre-shock = 2016–2018\n",
    "    is_post = (year >= 2019)  # Post-shock = 2019–2020\n",
    "\n",
    "    has_chinese_coauthor = any('CN' in auth['countries'] for auth in work['authorships'])\n",
    "    has_other_foreign_coauthor = any(\n",
    "        c not in ['US', 'CN'] for auth in work['authorships'] for c in auth['countries']\n",
    "    )\n",
    "\n",
    "    for authorship in work['authorships']:\n",
    "        author_id = authorship['author']['id']\n",
    "        if author_id in top_china_authors:\n",
    "            if is_pre:\n",
    "                author_metrics[author_id]['pre_pubs'] += 1\n",
    "                if has_chinese_coauthor:\n",
    "                    author_metrics[author_id]['pre_china_pubs'] += 1\n",
    "                if has_other_foreign_coauthor:\n",
    "                    author_metrics[author_id]['pre_other_foreign_pubs'] += 1\n",
    "                author_metrics[author_id]['pre_citations'].append(work['cited_by_count'])\n",
    "            elif is_post:\n",
    "                author_metrics[author_id]['post_pubs'] += 1\n",
    "                if has_chinese_coauthor:\n",
    "                    author_metrics[author_id]['post_china_pubs'] += 1\n",
    "                if has_other_foreign_coauthor:\n",
    "                    author_metrics[author_id]['post_other_foreign_pubs'] += 1\n",
    "                author_metrics[author_id]['post_citations'].append(work['cited_by_count'])\n",
    "\n",
    "# ========================\n",
    "# Step 3: Prepare clean table\n",
    "# ========================\n",
    "\n",
    "# Build a dataframe\n",
    "rows = []\n",
    "for author_id, metrics in author_metrics.items():\n",
    "    rows.append({\n",
    "        'author_id': author_id,\n",
    "        'pre_pubs': metrics['pre_pubs'],\n",
    "        'post_pubs': metrics['post_pubs'],\n",
    "        'pre_china_share': metrics['pre_china_pubs'] / metrics['pre_pubs'] if metrics['pre_pubs'] > 0 else np.nan,\n",
    "        'post_china_share': metrics['post_china_pubs'] / metrics['post_pubs'] if metrics['post_pubs'] > 0 else np.nan,\n",
    "        'pre_foreign_share': metrics['pre_other_foreign_pubs'] / metrics['pre_pubs'] if metrics['pre_pubs'] > 0 else np.nan,\n",
    "        'post_foreign_share': metrics['post_other_foreign_pubs'] / metrics['post_pubs'] if metrics['post_pubs'] > 0 else np.nan,\n",
    "        'pre_mean_citations': np.mean(metrics['pre_citations']) if metrics['pre_citations'] else np.nan,\n",
    "        'post_mean_citations': np.mean(metrics['post_citations']) if metrics['post_citations'] else np.nan,\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "# ========================\n",
    "# Step 4: Analysis\n",
    "# ========================\n",
    "\n",
    "# Change in collaborations\n",
    "df_metrics['delta_china_share'] = df_metrics['post_china_share'] - df_metrics['pre_china_share']\n",
    "df_metrics['delta_foreign_share'] = df_metrics['post_foreign_share'] - df_metrics['pre_foreign_share']\n",
    "\n",
    "# Change in publications\n",
    "df_metrics['delta_pubs'] = df_metrics['post_pubs'] - df_metrics['pre_pubs']\n",
    "\n",
    "# Change in citations\n",
    "df_metrics['delta_mean_citations'] = df_metrics['post_mean_citations'] - df_metrics['pre_mean_citations']\n",
    "\n",
    "# Summary\n",
    "print(df_metrics.describe())\n",
    "\n",
    "# You can plot histograms of the deltas:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df_metrics['delta_china_share'].dropna(), bins=20, alpha=0.7)\n",
    "plt.title('Change in China Collaboration Share (Post-Pre)')\n",
    "plt.xlabel('Delta Share')\n",
    "plt.ylabel('Number of Authors')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df_metrics['delta_foreign_share'].dropna(), bins=20, alpha=0.7)\n",
    "plt.title('Change in Other Foreign Collaboration Share (Post-Pre)')\n",
    "plt.xlabel('Delta Share')\n",
    "plt.ylabel('Number of Authors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to plot the graph with datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INSTALL IF NEEDED ===\n",
    "# !pip install -U datashader holoviews bokeh\n",
    "\n",
    "# === IMPORTS ===\n",
    "import holoviews as hv\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# === STEP 1: BUILD SPRING LAYOUT ===\n",
    "print(\"Computing layout for G_collaboration... (this can take a few minutes)\")\n",
    "try:\n",
    "    pos = nx.spring_layout(G_collaboration, seed=42, k=1/np.sqrt(G_collaboration.number_of_nodes()))\n",
    "except MemoryError:\n",
    "    print(\"MemoryError: using spectral layout instead...\")\n",
    "    pos = nx.spectral_layout(G_collaboration)\n",
    "\n",
    "# Create DataFrames for nodes and edges\n",
    "print(\"Building DataFrames...\")\n",
    "nodes_df = pd.DataFrame({\n",
    "    'x': [pos[node][0] for node in G_collaboration.nodes()],\n",
    "    'y': [pos[node][1] for node in G_collaboration.nodes()],\n",
    "})\n",
    "\n",
    "edges_x = []\n",
    "edges_y = []\n",
    "for u, v in G_collaboration.edges():\n",
    "    x0, y0 = pos[u]\n",
    "    x1, y1 = pos[v]\n",
    "    edges_x.append([x0, x1])\n",
    "    edges_y.append([y0, y1])\n",
    "\n",
    "edges_df = pd.DataFrame({\n",
    "    'x0': [p[0] for p in edges_x],\n",
    "    'y0': [p[0] for p in edges_y],\n",
    "    'x1': [p[1] for p in edges_x],\n",
    "    'y1': [p[1] for p in edges_y],\n",
    "})\n",
    "\n",
    "# === STEP 2: PLOT USING DATASHADER ===\n",
    "print(\"Rendering with Datashader...\")\n",
    "\n",
    "canvas = ds.Canvas(plot_width=1200, plot_height=1200)\n",
    "\n",
    "# Plot edges\n",
    "agg_edges = canvas.line(edges_df, 'x0', 'y0', 'x1', 'y1', agg=ds.count())\n",
    "\n",
    "# Plot nodes separately (optional, because usually edges dominate)\n",
    "agg_nodes = canvas.points(nodes_df, 'x', 'y', agg=ds.count())\n",
    "\n",
    "# Combine edges and nodes\n",
    "img_edges = tf.shade(agg_edges, cmap=[\"lightblue\", \"darkblue\"], how='log')\n",
    "img_nodes = tf.shade(agg_nodes, cmap=[\"orange\", \"red\"], how='log')\n",
    "\n",
    "final_img = tf.stack(img_edges, img_nodes)\n",
    "\n",
    "# Display\n",
    "hv.output(size=200)\n",
    "hv.Image(final_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
